import aiohttp
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import time
from utils.storage import load_cache, save_cache

async def fetch_articles_from_site(query=None, lang="ru", limit=5):
    start_time = time.time()
    base_url = "https://kadrovik.uz/" if lang == "ru" else "https://kadrovik.uz/uz/"
    url = base_url if not query else f"{base_url}search?q={query}"
    print(f"{datetime.now()}: –ù–∞—á–∞–ª–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ URL: {url}")
    
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        async with aiohttp.ClientSession() as session:
            async with session.get(url, headers=headers, timeout=aiohttp.ClientTimeout(total=5)) as response:
                response.raise_for_status()
                text = await response.text()
                soup = BeautifulSoup(text, "html.parser")

        articles = []
        # –ü–∞—Ä—Å–∏–Ω–≥ —Å–µ–∫—Ü–∏–∏ "–ù–æ–≤—ã–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏"
        posts_section = soup.select_one("section.posts-block ul.posts-list")
        if posts_section:
            for item in posts_section.select("li.post-card-wrapper")[:limit]:
                article_link = item.select_one("a[href]")
                if article_link:
                    title = item.select_one("h4.post-card__title")
                    date_elem = item.select_one("time.longread-post__time-published")
                    title_text = title.text.strip() if title else "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"
                    date_text = date_elem["datetime"] if date_elem else datetime.now().isoformat()
                    link = article_link["href"]
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π URL –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–π
                    if not link.startswith("http"):
                        link = base_url.rstrip("/") + "/" + link.lstrip("/")
                    articles.append({
                        "title": title_text,
                        "content": "",
                        "date": date_text,
                        "emoji": "üì∞",
                        "url": link
                    })

        if not articles:
            print(f"{datetime.now()}: –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å—Ç–∞—Ç—å–∏ –ø–æ URL: {url}")
        print(f"{datetime.now()}: –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. –í—Ä–µ–º—è: {time.time() - start_time:.2f} —Å–µ–∫. –ù–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(articles)}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à
        cache = load_cache()
        cache_key = f"latest_{lang}" if not query else f"search_{query}_{lang}"
        cache[cache_key] = {
            "timestamp": datetime.now().isoformat(),
            "data": articles
        }
        save_cache(cache)
        return articles
    except Exception as e:
        print(f"{datetime.now()}: –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Å–∞–π—Ç–∞: {e}. –í—Ä–µ–º—è: {time.time() - start_time:.2f} —Å–µ–∫")
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –µ—Å–ª–∏ –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ —É–¥–∞–ª—Å—è
        cache = load_cache()
        cache_key = f"latest_{lang}" if not query else f"search_{query}_{lang}"
        return cache.get(cache_key, {}).get("data", [])

async def search_articles(query, lang):
    cache = load_cache()
    cache_key = f"search_{query}_{lang}"
    
    if cache_key in cache:
        entry = cache[cache_key]
        timestamp = entry.get("timestamp")
        if timestamp and (datetime.now() - datetime.fromisoformat(timestamp)) < timedelta(hours=24):
            print(f"{datetime.now()}: –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: {query}")
            return entry["data"]

    print(f"{datetime.now()}: –ü–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: {query}")
    articles = await fetch_articles_from_site(query, lang)
    return articles

async def get_latest_articles(lang):
    cache = load_cache()
    cache_key = f"latest_{lang}"
    
    if cache_key in cache:
        entry = cache[cache_key]
        timestamp = entry.get("timestamp")
        if timestamp and (datetime.now() - datetime.fromisoformat(timestamp)) < timedelta(hours=24):
            print(f"{datetime.now()}: –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å—Ç–∞—Ç–µ–π ({lang})")
            return entry["data"]

    print(f"{datetime.now()}: –ü–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–∞ –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å—Ç–∞—Ç–µ–π ({lang})")
    articles = await fetch_articles_from_site(lang=lang)
    return articles