import aiohttp
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import time
from utils.storage import load_cache, save_cache

async def fetch_articles_from_site(query=None, lang="ru", limit=10):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Å—Ç–∞—Ç–µ–π —Å —Å–∞–π—Ç–∞ Kadrovik.uz"""
    start_time = time.time()
    base_url = "https://kadrovik.uz/" if lang == "ru" else "https://kadrovik.uz/uz/"
    url = base_url if not query else f"{base_url}search?q={query}"
    print(f"{datetime.now()}: –ù–∞—á–∞–ª–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ URL: {url}")
    
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        async with aiohttp.ClientSession() as session:
            async with session.get(url, headers=headers, timeout=aiohttp.ClientTimeout(total=6)) as response:
                response.raise_for_status()
                text = await response.text()
                soup = BeautifulSoup(text, "html.parser")

        articles = []
        posts_section = soup.select_one("section.posts-block ul.posts-list")
        if posts_section:
            for item in posts_section.select("li.post-card-wrapper")[:limit]:
                article_link = item.select_one("a[href]")
                if article_link:
                    title = item.select_one("h4.post-card__title")
                    date_elem = item.select_one("time.longread-post__time-published")
                    title_text = title.text.strip() if title else "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"
                    date_text = date_elem["datetime"] if date_elem else datetime.now().isoformat()
                    link = article_link["href"]
                    if not link.startswith("http"):
                        link = base_url.rstrip("/") + "/" + link.lstrip("/")
                    
                    articles.append({
                        "title": title_text,
                        "content": "",  # –£–±–∏—Ä–∞–µ–º –≤—ã–∑–æ–≤ fetch_article_content
                        "date": date_text,
                        "emoji": "üì∞",
                        "url": link
                    })

        if not articles:
            print(f"{datetime.now()}: –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å—Ç–∞—Ç—å–∏ –ø–æ URL: {url}")
        print(f"{datetime.now()}: –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. –í—Ä–µ–º—è: {time.time() - start_time:.2f} —Å–µ–∫. –ù–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(articles)}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
        cache = load_cache()
        cache_key = f"latest_{lang}" if not query else f"search_{query}_{lang}"
        cache[cache_key] = {
            "timestamp": datetime.now().isoformat(),
            "data": articles
        }
        save_cache(cache)
        return articles
    except Exception as e:
        print(f"{datetime.now()}: –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Å–∞–π—Ç–∞: {e}. –í—Ä–µ–º—è: {time.time() - start_time:.2f} —Å–µ–∫")
        cache = load_cache()
        cache_key = f"latest_{lang}" if not query else f"search_{query}_{lang}"
        return cache.get(cache_key, {}).get("data", [])

async def fetch_article_content(url):
    """–ü–∞—Ä—Å–µ—Ä —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–µ—Ä–µ–Ω–æ—Å–∞–º–∏ —Å—Ç—Ä–æ–∫ –ø–æ—Å–ª–µ emoji –∏ –∞–±–∑–∞—Ü–µ–≤"""
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.get(url, headers=headers, timeout=10) as response:
                response.raise_for_status()
                html = await response.text()
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # 1. –ó–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –¥–∞—Ç–∞ (–±–µ–∑ #)
        title = soup.find('h1').get_text(strip=True) if soup.find('h1') else "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"
        date_elem = soup.find('time', {'class': 'longread-post__time-published'})
        date = date_elem['datetime'] if date_elem else ""
        
        # 2. –ò—â–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º
        content_block = soup.find('section', {'class': 'longread-block'}) or soup.find('body')
        
        if not content_block:
            return f"üì∞ {title}\nüìÖ {date}\n\n–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç."
        
        # 3. –°–æ–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ <p> –∏ <strong>
        elements = content_block.find_all(['p', 'strong'])
        result = [f"üì∞ {title}", f"üìÖ {date}"]
        
        for element in elements:
            text = element.get_text(' ', strip=True)
            if text:
                if element.name == 'strong':
                    result.append(f"\nüîπ {text}\n")  # –ù–æ–≤—ã–π –∞–±–∑–∞—Ü –¥–ª—è üîπ
                else:
                    result.append(f"\n{text}")       # –ù–æ–≤—ã–π –∞–±–∑–∞—Ü –¥–ª—è –æ–±—ã—á–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        
        # 4. –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏ —É–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        content = ''.join(dict.fromkeys(result))
        
        return content if len(content) > 50 else "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç."
    
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞: {e}")
        return None

async def search_articles(query, lang):
    """–ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π –ø–æ –∑–∞–ø—Ä–æ—Å—É"""
    cache = load_cache()
    cache_key = f"search_{query}_{lang}"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
    if cache_key in cache:
        entry = cache[cache_key]
        timestamp = entry.get("timestamp")
        if timestamp and (datetime.now() - datetime.fromisoformat(timestamp)) < timedelta(hours=24):
            print(f"{datetime.now()}: –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: {query}")
            return entry["data"]

    print(f"{datetime.now()}: –ü–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: {query}")
    articles = await fetch_articles_from_site(query, lang)
    return articles

async def get_latest_articles(lang):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å—Ç–∞—Ç–µ–π"""
    cache = load_cache()
    cache_key = f"latest_{lang}"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
    if cache_key in cache:
        entry = cache[cache_key]
        timestamp = entry.get("timestamp")
        if timestamp and (datetime.now() - datetime.fromisoformat(timestamp)) < timedelta(hours=24):
            print(f"{datetime.now()}: –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å—Ç–∞—Ç–µ–π ({lang})")
            return entry["data"]

    print(f"{datetime.now()}: –ü–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–∞ –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å—Ç–∞—Ç–µ–π ({lang})")
    articles = await fetch_articles_from_site(lang=lang)
    return articles